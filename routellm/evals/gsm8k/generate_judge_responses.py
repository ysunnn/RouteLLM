import json
import requests
import time

import pandas as pd
from tqdm import tqdm

OLLAMA_URL = "http://localhost:11434/api/generate"


def _build_judging_prompt(user_prompt: str, week_response: str) -> str:
    return f"""
    You are an expert AI response evaluator. Assess the quality of the following response generated by LLM model.
    User Prompt:
    {user_prompt}

    Model Response:
    {week_response}

    Evaluate Response based on following metrics and give a score between 0.0 (worst) and 1.0 (best) representing
    the overall quality: accuracy, relevance, completeness, clarity, coherence, depth, conciseness, and style.
    Respond ONLY with a JSON object like: {{"score": 0.55}}.
    """


def query_ollama(prompt: str):
    payload = {
        "model": "llama3",
        "prompt": prompt,
        "stream": False
    }
    try:
        start_time = time.time()
        response = requests.post(OLLAMA_URL, json=payload)
        latency = time.time() - start_time

        response.raise_for_status()
        text_output = response.json()['response'].strip()
        score_data = json.loads(text_output)
        score = score_data.get("score", None)
        return score, latency
    except json.JSONDecodeError as e:
        print("JSON parsing error:", e)
        try:
            print("Raw response text:\n", response.text)
        except Exception as print_err:
            print("Could not print raw response:", print_err)
        return None, None
    except Exception as e:
        print(f"Error querying llama3: {e}")
        return None, None


def evaluate_and_save(csv_path: str):
    df = pd.read_csv(csv_path)

    scores = []
    latencies = []

    for _, row in tqdm(df.iterrows(), total=len(df)):
        prompt = row["prompt"]
        model_response = row["mistralai/Mixtral-8x7B-Instruct-v0.1_response"]
        judging_prompt = _build_judging_prompt(prompt, model_response)
        score, latency = query_ollama(judging_prompt)

        scores.append(score)
        latencies.append(latency)

    df["Mixtral-8x7B-Instruct-v0.1_response_judge_score"] = scores
    df["Mixtral-8x7B-Instruct-v0.1_response_judge_latency_sec"] = latencies

    df.to_csv(csv_path, index=False)
    print(f"Saved updated CSV to {csv_path}")


def evaluate_and_overwrite(csv_path: str):
    df = pd.read_csv(csv_path)

    # Initialize new columns if not present
    if "Mixtral-8x7B-Instruct-v0.1_response_judge_score" not in df.columns:
        df["Mixtral-8x7B-Instruct-v0.1_response_judge_score"] = None
    if "Mixtral-8x7B-Instruct-v0.1_response_judge_latency_sec" not in df.columns:
        df["Mixtral-8x7B-Instruct-v0.1_response_judge_latency_sec"] = None

    for i, row in tqdm(df.iterrows(), total=len(df)):
        # Skip rows that already have a score
        if pd.notnull(row["Mixtral-8x7B-Instruct-v0.1_response_judge_score"]):
            continue

        prompt = row["prompt"]
        model_response = row["mistralai/Mixtral-8x7B-Instruct-v0.1_response"]

        judging_prompt = _build_judging_prompt(prompt, model_response)
        score, latency = query_ollama(judging_prompt)

        df.at[i, "Mixtral-8x7B-Instruct-v0.1_response_judge_score"] = score
        df.at[i, "Mixtral-8x7B-Instruct-v0.1_response_judge_latency_sec"] = latency

    # Final save
    df.to_csv(csv_path, index=False)
    print(f"Completed update. CSV saved in-place to {csv_path}")


# evaluate_and_save("gsm8k_responses.csv")
evaluate_and_overwrite("gsm8k_responses.csv")
